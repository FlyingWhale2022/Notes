
---
tags: 专业课
aliases: 
创建时间: 2023-08-31 10:05
---


# 绪论

## 概论

模式识别的系统组成

![[Pasted Image 20230831111621_049.png|650]]

- 信息获取：对**信号、图像、数值表（数据）** 的采集。要**用计算机可以运算的符号**来表示所研究的对象。
- 预处理：**减小或消除**模式获取过程中的噪声、干扰，**加强有用信息**。
- 特征选择和变换
	- 特征选择：以**某种判决规则**为准则，**选取有效的特征**。
	- 变换：采用某种变换**对特征空间的压缩**，称为特征变换。
- 分类器设计：即所谓的**学习训练过程**。
	- 学习训练过程：用一定数量的样本**确定某种判别准则**，通过反复输入、修正，**直到分类错误率不超过给定值为止**（或错误率最小，或损失最小）。
	- 常见的训练学习方法：监督学习、无监督学习。
- 分类决策：**按分类判别准则**，将被识别模式**进行分类判决**，输出分类结果。


基本概念：
![image.png|500](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202308311150409.png)

>对对象的描述即为**模式**，表现的形式有：特征矢量、图、关系式等


## 特征矢量和特征空间

特征矢量：

![image.png|525](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202308311323780.png)

> 特征矢量即为原对象的**模式的一种表现形式**


特征空间：
![image.png|600](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202308311328228.png)

> 1、特征空间：**各种不同取值的 $x$ 的全体**构成了 $n$ 维空间。
> 2、特征点：特征矢量 $x$ 便是特征空间中的一个点，即特征点。
> 3、对某对象的**分类识别**是对其模式，即它的**特征矢量 $x$ 进行分类识别**。


## 随机矢量

定义：
由于量测系统随机因素的影响及同类不同对象的特征本身就是在特征空间散布的，**同一个对象或同一类对象的某特征量测值是随机变量**。由**随机分量**构成的矢量称为**随机矢量**。同一类对象的特征矢量在特征空间中是**按某种统计规律随机散布的**。

随机矢量的**分布函数**：
![image.png|525](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309011701538.png)

随机矢量的联合**概率密度函数**定义为：
![image.png|750](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309011703327.png)

随机矢量的期望：
![image.png|475](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309011704631.png)

随机矢量的条件期望：
![image.png|550](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309011706520.png)


一维随机矢量的正态分布概率密度函数：
![image.png|500](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309011710856.png)
![image.png|575](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309011711858.png)



# 基于贝叶斯决策理论的分类器

[[概率论知识补充#先验概率、后验概率]]

采用贝叶斯决策理论分类的**前提**：
目标（事物）的观察值是**随机的**，**服从一定的概率分布**。即：模式不是一个确定向量，而是一个**随机向量**。


## 后验概率

后验概率公式：
![image.png|600](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309021033340.png)

后验概率公式推导过程：
![[Excalidraw/模式识别.excalidraw.md#^area=P1lBgsqIaJAnkUh_px2WA|后验概率推导过程]]

## 两种常用的决策规则

### 最小错误率贝叶斯决策

**决策方法：**
![image.png|450](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309021043880.png)

**几种等价形式：**

![image.png|700](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309021050349.png)
> 常用第二种方法


### 最小风险贝叶斯决策

**损失函数：**
$$\lambda(\alpha_{i} ,w_{j})$$
其中：$w _ { j }$ 表示真实类别，$\alpha_{i}$ 表示决策也称判决，$\lambda_{ij}= \lambda(\alpha _{i},w_{j})$ 表示相应判决带来的损失

相关概念：

![image.png|625](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309021721833.png)


**条件平均风险计算公式：**

![image.png|600](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309021725853.png)

![image.png|600](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309021729961.png)

> 关于该公式的理解：$P(w_{j}|x)$ 表示作出决策 $\alpha_{i}$ 的概率，$\lambda(\alpha _{i},w_{j})$ 表示作出决策 $\alpha_{i}$ 但实际为 $w_{j}$ 时的风险。


**最小风险贝叶斯决策：**

![image.png|575](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309051052975.png)

> 就是取条件平均风险最小的那种决策


**实施最小风险判决规则的步骤:**

![image.png|675](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309051054260.png)

> 计算各类的后验概率 -> 根据决策表计算各决策的条件平均风险 -> 选择条件平均风险最小的那种决策

### 两种决策规则的关系

最小错误率贝叶斯决策规则是最小风险贝叶斯的**特例**。

![image.png|750](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309051310598.png)

最小错误率贝叶斯决策，也就**是 0-1 损失函数条件下**的最小风险贝叶斯决策。


## 分类器设计

c 类分类决策问题：按**决策规则**把 d 维特征空间**分为 c 个决策区域**。

**基本概念：**

- 决策面：**划分决策域的边界面**称为决策面。数学上用**决策面方程**表示。
- 判别函数：**表达决策规则**的函数，称为判别函数。


讨论具体的判别函数、决策面方程、分类器设计：

- **判别函数：**
![image.png|575](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309051345735.png)

![image.png|575](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309051346551.png)

- **决策面方程：**

![image.png|675](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309051348917.png)

![image.png|975](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309051356544.png)


## 正态分布时的统计决策

### 一维随机变量正态分布

![image.png|600](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309051909991.png)

### 多维随机变量正态分布

![image.png|700](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309061152345.png)

正态分布特点：

![image.png|600](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309061153050.png)

马氏距离：

![image.png|550](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309061154338.png)

等密度点轨迹：

![image.png|575](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309061347462.png)


### 正态分布下的最小错误率贝叶斯判别函数和决策面

#### 一般情况分析

**判别函数：**

![image.png|625](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309061446504.png)

![image.png|650](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309061454903.png)

**决策面方程：**

![image.png|675](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309111445590.png)

**例题：**

![image.png|675](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309111519816.png)
![image.png|600](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309111521445.png)


#### 特殊情况分析

 **情况 1：**

对所有类 $\color{red}{\sum_i=\sum_i=\sigma^2}\text{I}\quad i=1,...,c$ 

![image.png|600](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309172019492.png)
![image.png|600](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309061504825.png)

> 注意：当 $\color{red}{\sum_i=\sum_i=\sigma^2}\text{I}\quad i=1,...,c$ 时，等概率密度点为圆形


**情况 1.1：**

在情况 1 的基础上，当 $\color{red}\mathbf{P}(w_{i})=\mathbf{P}(w_{j})$ 时，判别函数进一步简化为：

$${g_i(x)=-\frac{\|x-u_i\|^2}{2\sigma^2}}$$

其中：$\begin{Vmatrix}x-\mu_i\end{Vmatrix}^2$ 为欧式距离

> 当满足 $p(x/w_i)$ 服从**正态分布**，且各类协方差矩阵 $\color{red}{\sum_i=\sum_i=\sigma^2}\text{I}\quad i=1,...,c$  ，同时**先验概率相等**时，此时的分类法则称为“最小距离分类法”。
> 
> 其判别规则为：
> 若 $\left\|x-\mu_{i}\right\|^{2}=\min_{j=1,\ldots,c}\left\|x-\mu_{j}\right\|^{2}$ ，则 $x\in w_{i}$
> 
> 这里解释一下为什么会是最小：因为判别函数始终为负（取了 $ln$，所以为负），要使后验概率最大，则 $\begin{Vmatrix}x-\mu_i\end{Vmatrix}^2$  得最小


**情况 1.2：**

在情况 1 的基础上，当 $\color{red}\mathrm{P}(w_{i})\neq\mathrm{P}(w_{j})$ 时，判别函数可化为：

$$g_i(x)=w_i^Tx+w_{io}$$
其中，$w_{i}=\frac{1}{\sigma^{2}}u_{i}$ ，$w_{io}=-\frac{1}{2\sigma^{2}}u_{i}^{T}u_{i}+\ln p(w_{i})$

> 此时判别函数称为**线性**判别函数，称为**线性分类器**


由决策面方程：$g_i(x)-g_j(x)=0$ 推出超平面方程：
$$w_{ij}^T(x-x_0)=0$$

其中：$w_{ij}=(u_i-u_j)$ ，$x_0=\dfrac{1}{2}(\mu_i+\mu_j)-\dfrac{\sigma^2\left(\mu_i-\mu_j\right)}{\left\|\mu_i-\mu_j\right\|^2}\text{ln}\dfrac{P(\omega_i)}{P(\omega_j)}$

**因此，决策面是一个通过 $x_0$，且与向量 $w$ 正交的超平面**

几何意义：

![image.png](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309180936015.png)



例题：

![image.png|625](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309111503791.png)

> 用欧氏距离判断即：样本更靠近哪一类的均值，即归于哪一类


