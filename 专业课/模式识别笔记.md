---
tags:
  - 模式识别
aliases: 
创建时间: 2023-08-31 10:05
---

# 资源汇总

📺 [青岛大学-模式识别](https://www.bilibili.com/video/BV1TE41197fc?t=1.5)

> 🥝 优点：
> 1、该视频讲解的比较全面，PPT 涉及到的知识点视频里都会讲解；
> 2、减轻看 ppt 的难度
> 
> 🍆 缺点：讲的比较简单

📖 [模式识别网上笔记](http://note.youdao.com/noteshare?id=28f1440ec969c81eb2f912baec23e15a)

> 🥝 优点：对每个章节的重点最后都会有总结，可根据这个进行快速复习
> 🍆 缺点：写的比较简略

📖 [[课程大纲2021.pdf]]

> 🥝 优点：课程的大纲，可根据这个捕捉重点


# 绪论

## 概论

模式识别的系统组成

![image.png](https://zbn-picture1-1319009493.cos.ap-chengdu.myqcloud.com/public-pic/202311261116493.png)


- 信息获取：对**信号、图像、数值表（数据）** 的采集。要**用计算机可以运算的符号**来表示所研究的对象。
- 预处理：**减小或消除**模式获取过程中的噪声、干扰，**加强有用信息**。
- 特征选择和变换
	- 特征选择：以**某种判决规则**为准则，**选取有效的特征**。
	- 变换：采用某种变换**对特征空间的压缩**，称为特征变换。
- 分类器设计：即所谓的**学习训练过程**。
	- 学习训练过程：用一定数量的样本**确定某种判别准则**，通过反复输入、修正，**直到分类错误率不超过给定值为止**（或错误率最小，或损失最小）。
	- 常见的训练学习方法：监督学习、无监督学习。
- 分类决策：**按分类判别准则**，将被识别模式**进行分类判决**，输出分类结果。


基本概念：
![image.png](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202308311150409.png)

>对对象的描述即为**模式**，表现的形式有：特征矢量、图、关系式等


## 特征矢量和特征空间

特征矢量：

![image.png](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202308311323780.png)

> 特征矢量即为原对象的**模式的一种表现形式**


特征空间：
![image.png](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202308311328228.png)

> 1、特征空间：**各种不同取值的 $x$ 的全体**构成了 $n$ 维空间。
> 2、特征点：特征矢量 $x$ 便是特征空间中的一个点，即特征点。
> 3、对某对象的**分类识别**是对其模式，即它的**特征矢量 $x$ 进行分类识别**。


## 随机矢量

定义：
由于量测系统随机因素的影响及同类不同对象的特征本身就是在特征空间散布的，**同一个对象或同一类对象的某特征量测值是随机变量**。由**随机分量**构成的矢量称为**随机矢量**。同一类对象的特征矢量在特征空间中是**按某种统计规律随机散布的**。

随机矢量的**分布函数**：

![image.png](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309011701538.png)

随机矢量的联合**概率密度函数**定义为：

![image.png](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309011703327.png)

随机矢量的期望：

![image.png](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309011704631.png)

随机矢量的条件期望：

![image.png](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309011706520.png)


一维随机矢量的正态分布概率密度函数：

![image.png](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309011710856.png)

![image.png](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309011711858.png)



# 基于贝叶斯决策理论的分类器

[[概率论知识补充#先验概率、后验概率]]

采用贝叶斯决策理论分类的**前提**：
目标（事物）的观察值是**随机的**，**服从一定的概率分布**。即：模式不是一个确定向量，而是一个**随机向量**。


## 后验概率

后验概率公式：

![image.png](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309021033340.png)

> 对后验概率的理解：
> 后验概率 $\left.p\left(\begin{array}{c|c}\omega_i&\vec{x}\end{array}\right.\right)$ 表示特征 $x$ 出现后，被划为 $\omega_i$ 的概率
> 对该公式的记忆：
> 公式的分子为：“该类的类概率密度 * 先验概率”
> 公式的分母为：“所有类的类概率密度 * 先验概率之和”

后验概率公式推导过程：

![[Excalidraw/模式识别.excalidraw.md#^area=P1lBgsqIaJAnkUh_px2WA|后验概率推导过程]]

## 两种常用的决策规则

### 最小错误率贝叶斯决策

**决策方法：**

![image.png](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309021043880.png)

> 哪个后验概率大，就把样本归为哪一类


**几种等价形式：**

![image.png](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309021050349.png)

> 常用第二种方法


**总结：**

最小错误率贝叶斯决策就是比较后验概率，因此把后验概率算出来是重中之重。



### 最小风险贝叶斯决策

**损失函数：**
$$\lambda(\alpha_{i} ,w_{j})$$
其中：$w _ { j }$ 表示真实类别，$\alpha_{i}$ 表示决策也称判决，$\lambda_{ij}= \lambda(\alpha _{i},w_{j})$ 表示相应判决带来的**损失**

相关概念：

![image.png](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309021721833.png)


**条件平均风险计算公式：**

![image.png](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309021725853.png)

![image.png](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309021729961.png)

> 关于该公式的理解：
> $R(\alpha_{i}\mid x)$ 表示根据特征 $x$ 作出决策 $\alpha_{i}$ 时带来的风险或者损失
> $P(w_{j}|x)$ 表示根据特征 $x$ 判别该样本为 $w_{j}$ 类的概率
> $\lambda(\alpha _{i},w_{j})$ 表示作出决策 $\alpha_{i}$ 但实际为 $w_{j}$ 时的风险。


**最小风险贝叶斯决策：**

![image.png](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309051052975.png)

> 就是取条件平均风险最小的那种决策


**实施最小风险判决规则的步骤:**

![image.png](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309051054260.png)

> 计算各类的后验概率 -> 根据决策表 得出 $\lambda(\alpha _{i},w_{j})$ -> 计算各决策的条件平均风险 -> 选择条件平均风险最小的那种决策

### 两种决策规则的关系

最小错误率贝叶斯决策规则是最小风险贝叶斯的**特例**。

![image.png](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309051310598.png)

最小错误率贝叶斯决策，也就**是 0-1 损失函数条件下**的最小风险贝叶斯决策。


## 分类器设计

c 类分类决策问题：按**决策规则**把 d 维特征空间**分为 c 个决策区域**。

**基本概念：**

- 决策面：**划分决策域的边界面**称为决策面。数学上用**决策面方程**表示。
- 判别函数：**表达决策规则**的函数，称为判别函数。


讨论具体的判别函数、决策面方程、分类器设计：

- **判别函数：**

![image.png](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309051345735.png)

![image.png](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309051346551.png)

- **决策面方程：**

![image.png](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309051348917.png)

![image.png](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309051356544.png)


## 正态分布时的统计决策

### 一维随机变量正态分布

![image.png](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309051909991.png)

### 多维随机变量正态分布

![image.png](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309061152345.png)

正态分布特点：

![image.png](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309061153050.png)

马氏距离：

![image.png](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309061154338.png)

等密度点轨迹：

![image.png](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309061347462.png)


### 正态分布下的最小错误率贝叶斯判别函数和决策面

#### 一般情况分析

**判别函数：**

![image.png](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309061446504.png)

![image.png](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309061454903.png)


**决策面方程：**

![image.png](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309111445590.png)


#### 特殊情况分析

 **情况 1：**

对所有类 $\color{red}{\sum_i=\sum_i=\sigma^2}\text{I}\quad i=1,...,c$ 

![image.png](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309172019492.png)
![image.png](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309061504825.png)

> 注意：当 $\color{red}{\sum_i=\sum_i=\sigma^2}\text{I}\quad i=1,...,c$ 时，等概率密度点为圆形


**情况 1.1：**

在情况 1 的基础上，当 $\color{red}\mathbf{P}(w_{i})=\mathbf{P}(w_{j})$ 时，判别函数进一步简化为：

$${g_i(x)=-\frac{\|x-u_i\|^2}{2\sigma^2}}$$

其中：$\begin{Vmatrix}x-\mu_i\end{Vmatrix}^2$ 为欧式距离

> 当满足 $p(x/w_i)$ 服从**正态分布**，且各类协方差矩阵 $\color{red}{\sum_i=\sum_i=\sigma^2}\text{I}\quad i=1,...,c$  ，同时**先验概率相等**时，此时的分类法则称为“最小距离分类法”。
> 
> 其判别规则为：
> 若 $\left\|x-\mu_{i}\right\|^{2}=\min_{j=1,\ldots,c}\left\|x-\mu_{j}\right\|^{2}$ ，则 $x\in w_{i}$
> 
> 这里解释一下为什么会是最小：因为判别函数始终为负（取了 $ln$，所以为负），要使后验概率最大，则 $\begin{Vmatrix}x-\mu_i\end{Vmatrix}^2$  得最小


**情况 1.2：**

在情况 1 的基础上，当 $\color{red}\mathrm{P}(w_{i})\neq\mathrm{P}(w_{j})$ 时，判别函数可化为：

$$g_i(x)=w_i^Tx+w_{io}$$
其中，$w_{i}=\frac{1}{\sigma^{2}}u_{i}$ ，$w_{io}=-\frac{1}{2\sigma^{2}}u_{i}^{T}u_{i}+\ln p(w_{i})$

> 此时判别函数称为**线性**判别函数，称为**线性分类器**


由决策面方程：$g_i(x)-g_j(x)=0$ 推出超平面方程：
$$w_{ij}^T(x-x_0)=0$$

其中：$w_{ij}=(u_i-u_j)$ ，$x_0=\dfrac{1}{2}(\mu_i+\mu_j)-\dfrac{\sigma^2\left(\mu_i-\mu_j\right)}{\left\|\mu_i-\mu_j\right\|^2}\text{ln}\dfrac{P(\omega_i)}{P(\omega_j)}$

**因此，决策面是一个通过 $x_0$，且与向量 $w$ 正交的超平面**

几何意义：

![image.png](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309180936015.png)

因为 $\sum_i=\sigma^2\text{I}\quad$，所以等概率面是一个圆形

因为 $W\text{与}(x-x_0)\text{点积为0},\quad\text{因此决策面}H\text{与}W\text{垂直}$

如果 $P(\omega_{1})=P(\omega_{2})$，$H$ 通过 $\mu$ 连线的中点

如果 $P(\omega_{1})\neq P(\omega_{2})$ ，$H$ 离开先验概率大的一类，即移向先验概率小的一类，如下图所示。

![image.png](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309180951371.png)


**情况 2：** 

当 $\color{red}{\sum_1=\sum_2=\ldots=\sum_M=\sum}$ 时

判别函数为：$g_{i}(x)=-\frac{1}{2}(x-\mu_{i})^{T}\sum^{-1}(x-\mu_{i})+\ln P(\omega_{i})$ 

**在此基础上**，当先验概率满足 $\color{red}{P (\omega_{1}})=P (\color{red}{\omega_{2}})=\ldots=P (\color{red}{\omega_{i}})$ 时，

判别函数为： $g_{i}(x)=-\frac{1}{2}(x-\mu_{i})^{T}\sum^{-1}(x-\mu_{i})$ 

该式包含**马氏距离**：$(x-\mu_i)^T\sum^{-1}(x-\mu_i)=r^2$ ，因此在该条件下，求样本 x 与各类均值的马氏距离，把 x 归于最近一类。（**最小距离分类器**）


同样，判别函数可化为如下**线性形式**：
$$g_{i}(x)=W_{i}^{T}x+w_{i0}$$
其中：$W_i=\sum^{-1}\mu_i$ ，$w_{i_0}=-\frac{1}{2}\mu_i^T\sum^{-1}\mu_i+\ln P(\omega_i)$

由决策面方程 $g_i(x)-g_j(x)=0$ 可得该条件下具体的决策面表达式：

$$W^{T}(x-x_{0})=0$$
其中：$W=\sum^{-1}(\mu_{i}-\mu_{j})$ ，$x_0=\dfrac{1}{2}(\mu_i-\mu_j)-\dfrac{\ln\dfrac{P(\omega_i)}{P(\omega_j)}(\mu_i-\mu_j)}{(\mu_i-\mu_j)^T\sum^{-1}(\mu_i-\mu_j)}$

> 这里只需关注决策面的表示形式即可，参数具体是什么不需要记


#### 例题

- 例题 1

![image.png](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309111503791.png)

> 这是特殊情况 1.1


- 例题 2

![image.png](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309111519816.png)
![image.png](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309111521445.png)
![image.png](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309191001755.png)
![image.png](https://zbn-picture1-1319009493.cos.ap-chengdu.myqcloud.com/public-pic/202309191044576.png)
![image.png](https://zbn-picture1-1319009493.cos.ap-chengdu.myqcloud.com/public-pic/202309191044483.png)


分析：

这里没有告诉两类的协方差关系，因此使用一般情况下的判别公式：

![image.png](https://zbn-picture-1319009493.cos.ap-guangzhou.myqcloud.com/public-pic/202309190955482.png)


这个公式需要计算向量 $\mu_1=(\mu_{11},\mu_{12})^T$ ，$\mu_2=(\mu_{21},\mu_{22})^T$ ，样本协方差 $\sum_1$ ，样本协方差 $\sum_2$ ，$|\sum_{1}|$ ，$|\sum_{2}|$，$\sum_1^{-1}$ ，$\sum_2^{-1}$ 

至于先验概率 $P(\omega_{1})$ 和 $P(\omega_{2})$ 由于题目没有告诉，因此由两类的样本的数量计算得到。

## 贝叶斯分类的算法流程

![image.png](https://zbn-picture1-1319009493.cos.ap-chengdu.myqcloud.com/public-pic/202309191049111.png)

## 概率密度函数的估计

贝叶斯分类器设计需要已知先验概率 $\mathbf{P}(\omega_{i})$ 和条件概率密度 $\mathbf{p(x/\omega_i)}$ ，这两个参数需要通过**样本训练估计**得来。由于先验概率 $\mathbf{P}(\omega_{i})$ 比较容易得到，**分类器训练**的主要任务是**确定类概密函数 $\mathbf{p(x/\omega_i)}$** 。

记估计值分别为： $\hat{p}(x/w_i)$ ，$\hat{P}(w_i)$

### 最大似然估计

**最大似然估计思想：**

把待估参数看作**确定性的量**，最佳估计就是**使训练样本出现概率为最大**的那个值，即让**似然函数达到最大**的参数值作为估计值

在最大似然估计中，每个类条件概密函数 **$\mathbf{p(x/w_{i})}$ 的形式已知**，**未知的是向量参数 $\theta_i$** 的值。

为强调 $\mathbf{p(x/w_{i})}$ 和 $\theta_i$ 有关，记为$\mathbf{p(x/w_i,\theta_i)}$

**注意：** 以下就只利用第  $i$ 类学习样本来估计第 $i$ 类的概率密度，**忽略类别标志**，即 $\mathbf{p(x/w_i,\theta_i)\to p(x/\theta)}$


**定义似然函数**：

设样本子集： $X=\{x_{1},x_{2},\ldots,x_{N}\}$（注意这里 $x_{N}$ 是样本，不是特征）

当样本是独立抽取的，则似然函数为:
$$p(X/\theta)=p(x_{1},x_{2},...,x_{N}\mid\theta)=\prod_{K=1}^{N}p(x_{K}/\theta)$$
> 这里运用到了联合概率密度函数的性质

对似然函数 $p(X/\theta)$ 的理解是：当选定 $\theta$ 时，样本子集 $X$ 出现的概率。

当 $X$ 的 $N$ 个样本确定后，**$p(X/\theta)$ 只是 $\theta$ 的函数**，记为 $l(\theta)$ ，使 $l(\theta)$ 达到**最大值**的的向量参数 $\hat{\theta}$ ，就是 $\theta$ 的最大似然估计

**计算方法：**

为便于分析，取对数形式：
$$H(\theta)=\ln l(\theta)=\ln p(X/\theta)=\sum_{k=1}^{N}\ln p(x_{k}/\theta)$$

然后对 $\theta$ 的每个分量求偏导，使其结果为 0，求出向量参数 $\hat{\theta}$ :
$$\frac{dH(\theta)}{d\theta}=0$$

若 $\theta$ 有 $s$ 个分量，则可得以下方程组：

$$\left.\left[\begin{aligned}&\sum_{k=1}^N\frac{\partial}{\partial\theta_1}\ln p(x_k\mid\theta)=0\\&\ldots\ldots\\&\ldots\ldots\\&\sum_{k=1}^N\frac{\partial}{\partial\theta_p}\ln p(x_k\mid\theta)=0\end{aligned}\right.\right.$$

$s$ 个联立方程组求解，可得 $\hat{\theta}$ 


**应用：**


 $\sum$  已知 ，$\text{μ}$ 未知，用最大似然估计可得：
 $$\hat{\mu}=\frac1N\sum_{k=1}^Nx_k$$
说明未知均值的**最大似然估计正好是训练样本的算术平均**


 $\sum$  ，$\text{μ}$ 均未知，对于**一维**情况，用最大似然估计可得：
 $${\hat{\mu}=\hat{\theta}_1=\frac1N\sum_{k=1}^Nx_k}$$
 $$\hat{\sigma}^2=\hat{\theta}_2=\frac{1}{N}\sum_{k=1}^{N}(x_k-\hat{\mu})^2$$


### 贝叶斯估计

基本思想：

与最大似然估计不同的是，$\theta$ 不再是确定量，而是**服从某种先验分布的随机量**，其**先验概率密度**为 $\mathbf{p}(\mathbf{\theta})$ 。利用已知的训练样本，求 $\theta$ 的后验概率密度 $\mathbf{p}(\mathbf{\theta}/\mathbf{X})$ 。

与贝叶斯决策不同是一个决策真实类别 $\mathbf{w}_{\mathbf{k}}$ ，一个是估计真实参数 $\theta$ 




# 线性分类器


用**样本集**设计**分类器两个步骤：**

- 确定判别函数**类型** ${g}_i(x)$ 
- 利用**样本集**确定判别函数的**未知参数**

---

以下针对**线性判别函数**进行讨论

线性判别函数定义：当 $g(x)$ 只是样本 $x$ 的**一次函数**

线性判别函数数学表达式：

$$\color{red}{g(x)=w^Tx+w_0}$$
![image.png](https://zbn-picture1-1319009493.cos.ap-chengdu.myqcloud.com/public-pic/202309251520602.png)

对于**两类**问题，判别函数可采用：
$$g(x)=g_1(x)-g_2(x)$$
> 注意 $w_i^T\neq w_j^T$  $w_{0i}\neq w_{0j}$
---

**决策面**：其中 $\mathbf{g(x)=0}$ 称为决策面 $H$

**决策界面 $H$ 的性质：**

- 决策面 $H$ 与权向量 $W$ 正交

![image.png](https://zbn-picture1-1319009493.cos.ap-chengdu.myqcloud.com/public-pic/202310082324204.png)

![image.png](https://zbn-picture1-1319009493.cos.ap-chengdu.myqcloud.com/public-pic/202310090032350.png)

> 公式里的 $w^T$ 不是 $w_i^T$ 也不是 $w_j^T$ ，而是 $g(x)=g_1(x)-g_2(x)$ 形成的一个新的线性判别函数里的 $w^T$。同理 $w_0$ 也是如此。
> $w$ 指向类别 $w_1$ 对应的区域

-  $g(x)$ 可看成是 $x$ 到 $H$ 面的距离的度量
$$\|r\|=\frac{g(x)}{\|w\|}$$

其中：$r$ 是 $x$ 到 $H$ 的垂直距离

- 原点到 $H$ 面的距离为 $r_0=\frac{w_0}{\|w\|}$ 

![image.png](https://zbn-picture1-1319009493.cos.ap-chengdu.myqcloud.com/public-pic/202309251543877.png)

结论：

![image.png](https://zbn-picture1-1319009493.cos.ap-chengdu.myqcloud.com/public-pic/202309251545973.png)


线性分类器的设计过程即：**寻找 $w$ 和 $w_0$ 的过程**

![image.png](https://zbn-picture1-1319009493.cos.ap-chengdu.myqcloud.com/public-pic/202309251546301.png)

> 这个结论比较重要

## 感知准则函数

### 概念介绍

**增广样本向量：**

$$\left.y=\left[\begin{matrix}1\\x\end{matrix}\right.\right]=\left[\begin{matrix}1\\x_1\\\vdots\\x_d\end{matrix}\right]$$

> 即是在 d 维特征向量上添加了一个“1”，变成 d+1 维，以便计算处理。

---

**权向量：**

$$a=\begin{bmatrix}w_0\\w\end{bmatrix}$$

此时判别函数可写为：
$$g(x)=a^Ty$$

为规范化分析，把来自 $w2$ 的样本模式均乘以(-1)，即：

$$y'_n=\begin{cases}y_i&\forall y_i\in w_1\\-y_j&\forall y_j\in w_2\end{cases}$$
> $y'_n$ 称为**规范化增广样本向量**

规范化后，只要寻找一个权向量 a，使全部样本 $y'_n$ 满足：

$$a^Ty’_n>0$$

此时的权向量称为**解向量** ，记为 $\text{a*}$ ，每个样本都对解向量的位置给出限制。

> 从图形的角度来看，解向量 $\text{a*}$ 与样本向量 $y’_n$ 的**夹角不能超过 90°**，因此可以说每个样本都对解向量的位置给出限制。

---

举例说明：

有4个训练样本，$\mathbf{w}_{1}=\{\mathbf{y}_{3},\mathbf{y}_{4}\}\quad,\mathbf{w}_{2}=\{\mathbf{y}_{1},\mathbf{y}_{2}\}$ ，将 ${w}_{2}$ 类的增广样本向量取负规范化，然后 4 个样本分别限制解向量的区间。

![image.png](https://zbn-picture1-1319009493.cos.ap-chengdu.myqcloud.com/public-pic/202310090722216.png)

### 感知准则函数

感知准则函数定义：

只考虑错分样本，即 $a^Ty’_n<0$ ，定义感知准则函数为：

$$J_p(a)=\sum_{y\in Y^K}(-a^Ty)$$
其中  $Y^K$ 是被权向量 $a$ 错分类的样本集合

当 $a$ 是解向量时，$J_p(a)$ 为最小。因此寻找解向量 $\text{a*}$ 的过程可以转化为寻找某向量 $a$ ，使 $J_p(a)$ 达最小值。**可采用梯度下降法求解**。

---

梯度下降法求解：

对 $a$ 求梯度：

$$\nabla J_p(a)=\frac{\partial J_p(a)}{\partial a}=\sum_{y\in Y^K}\left(-y\right)$$

梯度下降法的迭代公式：

$$a(k+1)=a(k)-\rho_k\nabla J=\alpha(k)+\rho_k\sum_{y\in Y^K}y$$

通常人为选定步长 $\rho_k$ ，这里设为 1

找 $a*$ 的迭代过程，可采用**单样本修正法**和**非单样本修正法**两种方法

- 单样本修正法

把样本看成一个**序列**，**逐个**加以考虑，每次迭代只使用一个样本。

举例说明：

![image.png](https://zbn-picture1-1319009493.cos.ap-chengdu.myqcloud.com/public-pic/202310090817875.png)
![image.png](https://zbn-picture1-1319009493.cos.ap-chengdu.myqcloud.com/public-pic/202310090819715.png)


- 非单样修正法

每次迭代必须遍历全部样本集，迭代公式为：

$$a_2=a_1+\color{red}{\sum y_i}$$

$y_i$ 为当解向量为 $a_k$ 时错分样本


迭代终止条件：

$a_{k}$  使所有样本正确分类（一般很难达到）或 $\left|\rho_k\nabla J_p(a_k)\right|<\theta$


## SVM 支持向量机

求解两类数据的最大间隔问题
间隔的正中即为决策超平面
间隔的边界称为正/负超平面，正/负超平面一定经过某些样本点，这些样本点称为支持向量
	
![image.png](https://zbn-picture1-1319009493.cos.ap-chengdu.myqcloud.com/public-pic/202310101126385.png)

![image.png](https://zbn-picture1-1319009493.cos.ap-chengdu.myqcloud.com/public-pic/202310101129319.png)


# 非线性分类器

## 前向多层神经网络

BP 网/前向多层神经网络定义：

除输入，输出层外，还**有一个或多个隐层**。各层之间的神经元**全互连**，各层内的神经元**无连接**，网络**无反馈**。

![image.png](https://zbn-picture1-1319009493.cos.ap-chengdu.myqcloud.com/public-pic/202310091443541.png)

节点的特性要求是可微的，通常选 S 型函数：

$$f(x)=\frac1{1+e^{-x}}$$

- 连续可微
- 单调
- 取值在[0,1]

---

BP 算法：对网络的**连接权值进行调整**，使得对任一输入都能得到所期望的输出。

BP 算法的主要步骤：

![image.png](https://zbn-picture1-1319009493.cos.ap-chengdu.myqcloud.com/public-pic/202310091455734.png)

相关参数

![image.png](https://zbn-picture1-1319009493.cos.ap-chengdu.myqcloud.com/public-pic/202310091516620.png)

$O_i$ : 节点 i 的输出;
$net_j$：节点 j 的输入；
$w_{ij}$：从节点 i 到节点 j 的连接权值
$$\mathrm{net}_\mathrm{j}=\sum_\mathrm{i}\mathrm{w}_\mathrm{ij}\mathrm{O}_\mathrm{i}$$

误差函数：
$$\mathrm{e}=\frac12\sum_\mathrm{k}\left(\hat{\mathrm{y}}_\mathrm{k}-\mathrm{y}_\mathrm{k}\right)^2$$
>  $\hat{\mathrm{y}}_\mathrm{k}$ 和 $\mathrm{y}_\mathrm{k}$ 分别表示输出层上第 k 个节点的期望输出与实际输出


连接权值的修正：

![image.png](https://zbn-picture1-1319009493.cos.ap-chengdu.myqcloud.com/public-pic/202310091531806.png)

$$\Delta w_{jk}=-\eta\frac{\partial e}{\partial w_{jk}}=-\eta\cdot\delta_{_k}\cdot O_{_j}$$ 
其中 $\eta$ 为增益因子，$\delta_\mathrm{k}=\frac{\partial e}{\partial net_k}$ 


分两种情况计算 $\delta_{\mathrm{k}}$ ：

- 当节点 k 为输出层上的节点时

$$\delta_{k}=-(\hat{y}_{k}-y_{k})\cdotp f^{'}(net_{k})$$
> $f^{'}()$ 为节点 k 的激活函数的导数

$$f^{'}(net_{k})=\mathrm{y}_k(1-\mathrm{y}_k)$$

- 节点不是输出层上的节点时

$$\delta_{k}=f^{'}(net_{k})\cdot\sum_{m}\delta_{m}w_{km}$$
> $\delta_{m}$ 表示下一层节点 $δ$ 值，$w_{km}$ 表示节点 $k$ 到下一层节点的连接权值
>故先计算最高层（输出层）上各节点 $δ$ 值，再反传到较低层上，计算各隐层节点的 $δ$ 值

$$f^{'}(net_{k})=O_{k}(1-O_{k})$$

算法流程图：

![image.png](https://zbn-picture1-1319009493.cos.ap-chengdu.myqcloud.com/public-pic/202310091604360.png)


# 其他分类方法

## 懒惰学习和急切学习介绍

 “懒惰学习”与“急切学习”概念介绍

“懒惰学习”（lazy learning）：此类学习技术**在训练阶段仅仅是把样本保存起来**，训练时间开销为零，待收到测试样本后再进行处理。

“急切学习”（eager learning）：**在训练阶段就对样本进行学习处理**的方法。

## 近邻法

💡 基本思想：对于一个新样本，逐一与已知样本比较，**找出距离新样本最近的已知样本**，以该样本的类别作为新样本的类别。

![image.png](https://zbn-picture1-1319009493.cos.ap-chengdu.myqcloud.com/public-pic/202310271132757.png)

---
🐳 对于该方法的说明：

![image.png](https://zbn-picture1-1319009493.cos.ap-chengdu.myqcloud.com/public-pic/202310271133022.png)

### K －近邻法

💡 K －近邻法基本思想

- 确定训练样本，以及某种距离度量，以及 k 值；
- 遍历每一个训练样本，计算测试样本到每一个训练样本的**距离**，按距离排序，**取 Top k 个最相近的训练样本**；
- 对于**分类问题**使用“**投票法**”获得预测结果；对于**回归问题**使用“**平均法**”获得预测结果。还可**基于距离远近**进行**加权平均或加权投票**，距离越近的样本权重越大。

> 投票法：选择这 k 个样本中**出现最多的类别**标记作为预测结果。（本章学习中采用该决策机制）
> ❓ 平均法：将这 k 个样本的**实值输出标记的平均值**作为预测结果。（本章学习中没有用）

🌟 K 近邻学习没有显式的训练过程（在没有测试样本之前，不对训练样本进行学习处理），属于“懒惰学习”

---
🐳  K －近邻法的三个基本要素

- 距离度量
- k 值的选择
- 分类决策规则决定

🍒 K 值的选择

![image.png](https://zbn-picture1-1319009493.cos.ap-chengdu.myqcloud.com/public-pic/202311281054142.png)

---
🐳  K －近邻法的缺点

- 存储量和计算量大
- 票数接近时风险较大
- 有躁声时风险加大


## 决策树

#### 基本概念

📺 [决策树原理介绍](https://www.bilibili.com/video/BV1TE41197fc?t=13.7&p=34)

🌾 作用：直接利用**非数值特征**进行分类的方法

---
🐳 决策树特征：

是由一系列节点组成，**每一个节点代表一个特征/属性**，根据该节点将一个数据集分裂成几个子数据集。

![image.png](https://zbn-picture1-1319009493.cos.ap-chengdu.myqcloud.com/public-pic/202311291405723.png)

如上图，对数据集首先根据形状分为两个子数据集，然后将圆形那个数据集根据颜色继续划分为两个子数据集，……，直到最后不能划分，得到一系列叶节点，即最后的分类结果。

> 由此可以看出

---
💡 算法思想：

模型训练（即构造决策树）：开始时所有数据在根节点，然后选择属性（每个节点代表一个属性）不断对测试数据集进行划分，得到一系列叶节点，叶节点即为最后的分类结果

测试时：将测试样本从第一个节点出发，按照每个节点属性进行划分，最后该样本会归类到某个叶节点从而完成分类。

> 由此可以看出决策树有显式的训练过程（在测试样本之前，对训练样本进行学习处理），属于“急切学习”

---
🐳 决策树的两个中止条件：

- 一个节点上的数据属于同一个类别
- 没有属性可以再用于分割

### ID3 方法

📺 [ID3 方法](https://www.bilibili.com/video/BV1TE41197fc?t=28.9&p=35)

🌾 ID3 方法的目的：

选择属性进行分裂，使**子结点数据类值尽可能相同**（即得到尽可能纯的子结点）。

---
💡 基本思想：

🍒 依据：**信息熵越小**，子结点数据类值尽可能相同（即子节点的就越纯）。

🍒 算法思路：

因此我们可以先计算数据集的**初始信息熵**，然后计算**每个属性的信息熵**。数据集的信息熵减去每个属性的信息熵得到每个属性的**信息增益**。选择**信息增益最大**的属性作为该节点的分裂属性。

分裂后得到几个子集，如果子集是**纯节点**，则停止；如果**没有属性**可以再用于分割，则停止；如果子集是**非纯节点且有属性**可以再用于分割，则利用上述相同方法对该数据集选择信息增益最大的分裂属性。

按照上述方法不断分裂，直到满足决策树的两个中止条件，则停止。

---
🐳 信息熵/香农熵

$I=-(P_1\log_2P_1+P_2\log_2P_2+\cdots+P_k\log_2P_k)=-\sum_{i=1}^kP_i\log_2P_i$
其中，$P_i$ 为数据集中第 $i$ 类出现的概率

---
🐳 计算数据集的初始信息熵

📺 [计算数据集的初始信息熵](https://www.bilibili.com/video/BV1TE41197fc?t=386.0&p=35)

![image.png](https://zbn-picture1-1319009493.cos.ap-chengdu.myqcloud.com/public-pic/202311291344786.png)

整个数据集中分为两类，用这两类的概率计算初始信息熵

> $info([9,5])$ 的意思是：数据集一共两类，一类数量为 9 个，另一类数量为 5 个，从而计算信息熵

---
🐳 计算每个属性的信息增益

📺 [计算每个属性的信息增益](https://www.bilibili.com/video/BV1TE41197fc?t=439.1&p=35)

![image.png](https://zbn-picture1-1319009493.cos.ap-chengdu.myqcloud.com/public-pic/202311291347410.png)

一共有四个属性，每个属性都可以产生若干个子集，这里以"outlook"属性为例进行讲解

![image.png](https://zbn-picture1-1319009493.cos.ap-chengdu.myqcloud.com/public-pic/202311291349124.png)

"outlook"属性分裂成 3 个数据子集，每个数据子集都包含两类：Yes 和 No 。计算每个子集的信息熵 $info([2,3])$，$info([4,0])$，$info([3,2])$。

则该属性的信息熵为各个子集信息熵的加权平均：

![image.png](https://zbn-picture1-1319009493.cos.ap-chengdu.myqcloud.com/public-pic/202311291356768.png)

该属性的信息增益为数据集的信息熵减去属性的信息熵：

![image.png](https://zbn-picture1-1319009493.cos.ap-chengdu.myqcloud.com/public-pic/202311291358410.png)


# 非监督模式识别

## 基于概率分布模型的方法

该种方法需要依据概率密度函数。

### 单峰子集分离法

💡 基本思想：

![image.png](https://zbn-picture1-1319009493.cos.ap-chengdu.myqcloud.com/public-pic/202312011655384.png)

![image.png](https://zbn-picture1-1319009493.cos.ap-chengdu.myqcloud.com/public-pic/202312011701802.png)

---
🌵 举例说明

![image.png](https://zbn-picture1-1319009493.cos.ap-chengdu.myqcloud.com/public-pic/202312011703546.png)

> 如果存在第三主成分，则依次对 A、B、C 三个子集在第三主成分方向上进行投影。如出现波谷，则可以继续划分出其它类别；如果在在该维上只有一个峰，则没有新的类别出现。


### EM 算法

#### EM 算法介绍

📺 [EM 算法介绍](https://www.bilibili.com/video/BV1pQ4y1s7oG?t=7.7)

🍃 EM 算法（期望最大化算法）的概念：

是一种从**不完全数据或有数据丢失的数据集（存在隐含变量）** 中求解概率模型参数的最大似然估计方法。

概念要点：
- 存在隐变量的数据集（在无监督中，隐变量是指无标签训练集通过某种划分方法后得到的各个类别）
- 是一种最大似然估计方法，即通过训练数据集求解模型参数

---
🍂 引例

![image.png](https://zbn-picture1-1319009493.cos.ap-chengdu.myqcloud.com/public-pic/202311301425767.png)

---
🐳 EM 算法思路

![image.png](https://zbn-picture1-1319009493.cos.ap-chengdu.myqcloud.com/public-pic/202311301430429.png)

🌟总结算法流程：
- 设定初始参数
- 计算隐藏变量的后验概率
- 根据计算后的结果更新参数
- 不断迭代……

---
🍂 引例

![image.png](https://zbn-picture1-1319009493.cos.ap-chengdu.myqcloud.com/public-pic/202311301438906.png)


#### EM 算法应用于混合高斯模型（GMM）

📺 [ EM 算法应用于混合高斯模型](https://www.bilibili.com/video/BV1RT411G7jJ?t=3.1)

##### 高斯混合模型

📺 [高斯混合模型介绍](https://www.bilibili.com/video/BV1hZ4y1x7BL?t=13.0)

🍃 高斯混合模型形式：简单来说就是多个高斯模型（正态分布）的线性组合。

🔥 根据视频理解以下两张图片中的内容即可。

![image.png](https://zbn-picture1-1319009493.cos.ap-chengdu.myqcloud.com/public-pic/202311301345796.png)

![image.png](https://zbn-picture1-1319009493.cos.ap-chengdu.myqcloud.com/public-pic/202311301347361.png)

❓ 如何对高斯混合模型建立**无监督**分类模型

在训练时如果计算出 $p(k|x)$ 就可以对无标签的训练样本进行分类，从而确定训练样本的标签。但是公式中的 ${\mu}$ 、${\Sigma}$ 和 $\pi$ 也是待估计值。如果采用极大似然法对参数进行估计，需要知道训练样本的标签，然而我们这是无监督模型，样本都没有标签，这就会造成冲突。

解决的办法是：给定 ${\mu}$ 、${\Sigma}$ 和 $\pi$ 初值，计算出 $p(k|x)$ 后再来更新 ${\mu}$ 、${\Sigma}$ 和 $\pi$ 的值，这样不断迭代直到趋于稳定，这样的方法叫“EM 算法”。

#####  EM 算法处理混合高斯模型

![image.png](https://zbn-picture1-1319009493.cos.ap-chengdu.myqcloud.com/public-pic/202311301444902.png)
![image.png](https://zbn-picture1-1319009493.cos.ap-chengdu.myqcloud.com/public-pic/202311301446653.png)

> 参考视频会帮助理解


## 基于相似性度量的方法（聚类方法）

### 动态聚类方法

🐳 任务：
- 将数据集划分成一定数量的子集（**确定子集数目**）
- 找到当前数据集最佳的划分（这里有一个**动态迭代**过程，因此叫动态聚类方法）

---
💡基本思路

首先将一个数据集初始划分一定数量的聚类，然后找出各个聚类的代表点，对于其它点依次计算到这些代表点的距离，并将点分配到距离最短的那一类。

---
🐳要点：
- 选定**某种距离度量**（如欧氏距离）作为相似性度量
- 确定样本合理的**初始分类（初始代表点）**（初始状态可以作为迭代的第一个状态）
- 确定某种评价聚类质量的**准则函数**（准则函数用来调整聚类划分，需要通过不断迭代从而达到准则函数的极值）

#### K 均值（C 均值）

K 均值（或称 C 均值）法是经典的动态聚类方法。

🐳 相似性度量选择

通常选择**欧式距离**

---
🐳 初始分类（代表点选择）

- 凭经验选择
- 随机选择

---
🐳 准则函数

K 均值采用的是**误差平方和准则**

$$J_{e}=\sum_{i=1}^{c}\sum_{y\in C}d(y,m)=\sum_{i=1}^{c}J_{i}$$

其中，$J_{e}$ 表示准则函数，$c$ 代表聚类数量，$y$ 表示样本， $m$ 表示聚类均值， $d(y,m)$ 样本到其所在聚类均值的距离。

🌟 准则函数相当于所有的样本到其所在聚类的均值的距离之和。当 $J_{e}$ 最小时候可以达到最优划分，也就是**类内样本相似度极高，类与类之间的样本相似度非常低**。

---
🐳 C 均值算法的基本步骤


# 特征选择与提取

## 基本概念

🐳 特征的基本概念

特征：描述物体的属性

特征的分类：
- 相关特征：对**当前学习任务**有用的属性
- 无关特征：与**当前学习任务**无关的属性
- 冗余特征：其所包含信息能由其他特征推演出来

---
🐳 特征相关操作

- 特征形成：由仪器直接测量出来的数值，或者是根据仪器的数据进行计算后的结果。
- 特征选择：用计算的方法从一组给定的特征中选择一部分特征进行分类。（降低特征维数的基本方法）
- 特征提取：通过适当的变换把原有的 D 个特征转换为 d（<D）个特征。（**降低特征维数，还可以消除特征之间存在的相关性**，进而减少特征中与分类无关的信息，使得新的特征更有利于分类）

> 🔥 注意“特征选择”和“特征提取”的区别


## 特征选择

- 特征的评价：怎样衡量一组特征对分类的有效性（**评价准则**）
- 寻优的算法：怎样更快地找到性能最优或比较优的特征组合

> 总结：确定了评价准则后 （特征的评价），特征选择问题就变成从 D 个特征中选择出**使评价准则最优**的 d 个特征（d＜D）的搜索问题（寻优的算法）。

### 特征的评价准则 (特征选择判据)

❌ 利用分类器的错误率作为准则是最直接的想法，但是**不可行**（计算复杂度高）。
✔️ 因此定义**与错误率有一定关系但又便于计算**的类别可分性准则 $J_{ij}$，用来衡量在**一组特征下第 $i$ 类和第 $j$ 类之间的可分程度**。

对判据的要求：

![image.png](https://zbn-picture1-1319009493.cos.ap-chengdu.myqcloud.com/public-pic/202311151403877.png)

以下将介绍几个常用的特征选择判据。

#### 基于类内类间距离的可分性判据

🍂 各类样本可以分开，是因为它们位于特征空间的不同区域，显然这些区域之间距离越大，类别可分性就越好。

🐳 基本思想：

以各类特征向量之间的平均距离 $J_D$ 作为判据，$J_D$ 越大，可分性越好

> 考虑最简单的两类情况，可以用两类中任意两两样本间的平均来代表两个类之间的距离。

---
🐳 类内离散度矩阵 ${S}_{w}$ 和类间离散度矩阵 $S_b$ 的估计

📺 [讲解视频](https://www.bilibili.com/video/BV1TE41197fc?t=83.2&p=40)

![image.png](https://zbn-picture1-1319009493.cos.ap-chengdu.myqcloud.com/public-pic/202311151455789.png)

---
🐳 常用的基于类内类间距离的可分性判据（以两类为例）

$$J_1=\mathrm{tr}\big(\mathrm{S}_\mathrm{w}+S_b\big)$$
> 平均平方距离判据

$$J_{_2}=\mathrm{tr}\left(S_{_w}^{-1}S_{_b}\right)$$ $$J_3=\ln\frac{\left|S_b\right|}{\left|S_w\right|}$$
$$J_4=\frac{\mathrm{tr}S_b}{\mathrm{tr}S_w}$$

> 🔥 注意： $tr$ 表示矩阵的迹

---
🌟 总结：

特点：
- 基于类内类间距离的可分离性判据实际上是**各类向量之间的平均距离**。
- $J_{(x)}$ 越大，可分离性越好

优点：
- 直观，易于计算

缺点：
- 没有考虑各类分布重叠情况且和错误率没有直接关系
- 缺点是当类间距离较小，类内距离较大时，判据仍有可能取得较大的值，而此时的可分离性并不大。

判据适用范围：
当各类协方差相差不大时，用此种判据较好。


#### 基于概率分布的可分性判据




## 特征提取（特征变换）

🐳 特征提取思想

通过适当的变换把 D 个特征转换为 d 个新特征，具体来说是指**从一组已有的特征通过一定的数学运算得到一组新特征**，有时也把这种特征提取称为**特征变换**。

> 🌾作用：
> 1、可以降低维度
> 2、消除特征之间可能存在的相关性，进而减少特征中与分类无关的信息，使得新特征更有利于分类

🐳 
- 线性特征变换：PCA、K－L 变换
$$y=W^Tx$$
- 非线性特征变换：Manifold Learning
$$y=W(x)$$
